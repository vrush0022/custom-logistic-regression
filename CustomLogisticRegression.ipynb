{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics,linear_model,preprocessing,model_selection\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.intercepts = None\n",
    "        \n",
    "    def __calcProbability(self,iv,params):\n",
    "        return(1/(1+np.exp(-np.dot(params.T,iv))))\n",
    "\n",
    "    def predict(self,test_X,prob_threshold=0.5):\n",
    "        #Threshold not required for Multiclass Classification\n",
    "        if(self.intercepts==None or len(self.intercepts)==0 ):\n",
    "            raise Exception('Model not fitted.')\n",
    "            \n",
    "        test_X=np.column_stack((np.ones(len(test_X)),test_X))\n",
    "        final_prediction=[]\n",
    "        if len(self.intercepts)>1:#Multiclass classification\n",
    "            probsList=[]      \n",
    "            #Predict the probability using each model\n",
    "            class_labels=[]\n",
    "            for cl,params in self.intercepts.items():                  \n",
    "                probs=np.apply_along_axis(self.__calcProbability,1,test_X,params)#using each model predict the probability\n",
    "                class_labels.append(cl)#keep list of all the class labels\n",
    "                probsList.append(probs)\n",
    "            #Check which model gave highest probability to class 1 and use the corresponding class label.\n",
    "            final_prediction=np.apply_along_axis(lambda x:class_labels[np.argmax(x)],0,probsList)\n",
    "            \n",
    "        else:# Binary classification\n",
    "            probs=np.apply_along_axis(self.__calcProbability,1,test_X,self.intercepts.get(1))  \n",
    "            probs[probs>=prob_threshold]=1\n",
    "            probs[probs<prob_threshold]=0\n",
    "            final_prediction=probs\n",
    "        return(final_prediction)\n",
    "\n",
    "    def __gradientDescent(self,x,y,no_iter,learning_rate,stochastic,lambda_val):\n",
    "        N=len(x)\n",
    "        x_actual=np.column_stack((np.ones(N),x))#add new column having values 1 for y intercept parameter\n",
    "        y_actual=y\n",
    "        params=np.zeros(x_actual.shape[1]) #initialize the parameters to 0. i.e B0=B1=B2=0\n",
    "        for i in range(no_iter):\n",
    "            if(stochastic):\n",
    "                random_num=random.sample(range(0, N), 1)            \n",
    "                x_temp=x_actual[random_num,:]\n",
    "                y_temp=y_actual[random_num]        \n",
    "            gradients=np.zeros(x_temp.shape[1])#initialize gradients    \n",
    "            #predict probability for each sample. apply function along each row\n",
    "            all_pred=np.apply_along_axis(self.__calcProbability,1,x_temp,params)\n",
    "            for i in range(x_temp.shape[1]):\n",
    "                #calculate gradient of each feature                     \n",
    "                grad=sum((all_pred-y_temp)*x_temp[:,i])+ 2*lambda_val*params[i]\n",
    "                gradients[i]=grad\n",
    "            \n",
    "            #recalculate new parameters\n",
    "            params=params-(learning_rate*gradients)\n",
    "        return params\n",
    "\n",
    "    def fit(self,train_X, train_y, alpha=0.01, num_iterations=10000, lambda_reg=0.0001,stochastic=True):\n",
    "        \n",
    "        classes=np.unique(train_y)\n",
    "        models={}\n",
    "        if(len(classes)>2):            \n",
    "            #One vs Rest Approach.\n",
    "            #A model will be created for each class. with 1 label being that of target class\n",
    "            #and all the other classes will be class 0\n",
    "            for cl in classes:\n",
    "                ty=train_y.copy() \n",
    "                ty[ty!=cl]=0\n",
    "                ty[ty==cl]=1\n",
    "                #Train each of the model using gradient descent\n",
    "                #store the trained coefficients in a dictionary with the key being the actual label of class 1\n",
    "                params=self.__gradientDescent(train_X,ty,num_iterations,alpha,stochastic,lambda_reg)\n",
    "                models.update({cl:params})\n",
    "               \n",
    "        else:\n",
    "            params=self.__gradientDescent(train_X,train_y,num_iterations,alpha,stochastic,lambda_reg)\n",
    "            models.update({1:params})\n",
    "        self.intercepts=models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOAD DATASET'''\n",
    "covtype=datasets.fetch_covtype()\n",
    "x = covtype.data\n",
    "y = covtype.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''split into train and test'''\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y,random_state = 18,test_size=0.3)\n",
    "x_train=preprocessing.scale(x_train)\n",
    "x_test=preprocessing.scale(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As I have used Stochastic Gradient Descent, the number of iterations is relatively large. Also as the model was underfitting,\n",
    "    I have kept the regularization parameter small. The learning rate has been set to 0.01 after trying different values</b>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON TRAIN 0.704753779124089\n",
      "ACCURACY ON TEST 0.7063463833302736\n"
     ]
    }
   ],
   "source": [
    "'''Test Custom Logistic Regression'''\n",
    "model=MyLogisticRegression()            \n",
    "#Tuned Parameters. As I have used stochastic GD, the number of iterations is high\n",
    "model.fit(x_train,y_train,alpha=0.01,num_iterations=30000,lambda_reg=0.001)\n",
    "y_pred=model.predict(x_train)\n",
    "print('ACCURACY ON TRAIN',metrics.accuracy_score(y_pred,y_train))\n",
    "y_pred_test=model.predict(x_test)    \n",
    "print('ACCURACY ON TEST',metrics.accuracy_score(y_pred_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON TRAIN 0.7148347217168091\n",
      "ACCURACY ON TEST 0.7156404901780796\n"
     ]
    }
   ],
   "source": [
    "'''Test using Sklearn Logistic Regression'''\n",
    "sklearn_logisticmodel=linear_model.LogisticRegression()#Default is ridge in sklearn. \n",
    "sklearn_logisticmodel.fit(x_train,y_train)    \n",
    "sklearn_pred_train=sklearn_logisticmodel.predict(x_train)\n",
    "print('ACCURACY ON TRAIN',metrics.accuracy_score(sklearn_pred_train,y_train))\n",
    "sklearn_pred_test=sklearn_logisticmodel.predict(x_test)    \n",
    "print('ACCURACY ON TEST',metrics.accuracy_score(sklearn_pred_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_logisticmodel.get_params# we can increase the C value to reduce the regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Changing the solver to Stochastic Average Gradient Descent and using multinomial instead of ovr</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gudea\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON TRAIN 0.7243919470480049\n",
      "ACCURACY ON TEST 0.7242633559757665\n"
     ]
    }
   ],
   "source": [
    "'''Tuned SKLEARN Logistic Regression'''\n",
    "sklearn_logisticmodel2=linear_model.LogisticRegression(multi_class='multinomial',solver='saga',C=100)\n",
    "sklearn_logisticmodel2.fit(x_train,y_train)    \n",
    "sklearn_pred_train2=sklearn_logisticmodel2.predict(x_train)\n",
    "print('ACCURACY ON TRAIN',metrics.accuracy_score(sklearn_pred_train2,y_train))\n",
    "sklearn_pred_test2=sklearn_logisticmodel2.predict(x_test)    \n",
    "print('ACCURACY ON TEST',metrics.accuracy_score(sklearn_pred_test2,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can conclude that the results of custom logistic regression model is quite similar to that of Sklearn implementation. We\n",
    "    can further optimize it by using parallel processing and softmax instead of ovr.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
